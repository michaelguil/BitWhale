{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP1HfUUdG1JDLmiy+nYkc99"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"g2YGYLMEpiUS"},"outputs":[],"source":["import pandas as pd\n","import os\n","\n","# 1. TSV íŒŒì¼ì´ í¬í•¨ëœ í´ë”ì—ì„œ ëª¨ë“  íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜\n","def load_tsv_files_from_folder(folder_path=\"C:/python/python(1-0)TSV\"):\n","    # í´ë” ë‚´ì˜ ëª¨ë“  .tsv íŒŒì¼ì„ ì°¾ìŒ\n","    tsv_files = [f for f in os.listdir(folder_path) if f.endswith('.tsv')]\n","    return tsv_files\n","\n","# 2. TSV íŒŒì¼ ì½ê¸° ë° ë°ì´í„° ì²˜ë¦¬\n","def process_tsv_file(file_path, output_csv=\"(1-1)ëª¨ë“  transactions.csv\"):\n","    # 2. TSV íŒŒì¼ ì½ê¸°\n","    df = pd.read_csv(file_path, sep='\\t')\n","\n","    # 3. 'input_total' ì—´ì´ ìˆëŠ”ì§€ í™•ì¸ í›„, í•„í„°ë§\n","    if 'input_total' in df.columns:\n","        # 'input_total'ì´ ë¬¸ìì—´ë¡œ ë˜ì–´ìˆë‹¤ë©´, ì´ë¥¼ ìˆ«ìë¡œ ë³€í™˜ (ë‹¨ìœ„ëŠ” Satoshis)\n","        df['input_total'] = pd.to_numeric(df['input_total'], errors='coerce')\n","\n","        # Satoshisë¥¼ BTCë¡œ ë³€í™˜ (1 BTC = 100,000,000 satoshis)\n","        df['input_total'] = df['input_total'] / 1e8  # Satoshi -> BTC\n","\n","        # 3-1. 10 BTC ë¯¸ë§Œì¸ í–‰ì„ ì‚­ì œ\n","        df = df[df['input_total'] >= 10]\n","    else:\n","        print(f\"íŒŒì¼ì— 'input_total' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n","        return None\n","\n","    # 4. hashì™€ time ë°ì´í„°ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ëª¨ë“  ë°ì´í„° ì‚­ì œ\n","    if 'hash' in df.columns and 'time' in df.columns:\n","        df = df[['hash', 'time']]  # 'hash'ì™€ 'time' ì—´ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ\n","    else:\n","        missing_columns = [col for col in ['hash', 'time'] if col not in df.columns]\n","        print(f\"íŒŒì¼ì— ë‹¤ìŒ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_columns)}\")\n","        return None\n","\n","    # 5. íŠ¸ëœì­ì…˜ hashì™€ time ë°ì´í„°ë¥¼ 'í˜„ë¬¼ large_transactions-1.csv'ì— ì¶”ê°€ ì €ì¥\n","    if not os.path.exists(output_csv):  # íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” í¬í•¨\n","        df.to_csv(output_csv, index=False, mode='w', encoding='utf-8')\n","    else:  # íŒŒì¼ì´ ìˆìœ¼ë©´ í—¤ë” ì œì™¸\n","        df.to_csv(output_csv, index=False, mode='a', header=False, encoding='utf-8')\n","\n","    print(f\"{output_csv} íŒŒì¼ì— íŠ¸ëœì­ì…˜ hashì™€ time ë°ì´í„°ë¥¼ ì¶”ê°€ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n","\n","# 6. í´ë” ë‚´ ëª¨ë“  TSV íŒŒì¼ ì²˜ë¦¬\n","def process_files_in_folder(folder_path=\"C:/python/python(1-0)TSV\"):\n","    # í´ë” ë‚´ ëª¨ë“  .tsv íŒŒì¼ì„ ì°¾ìŒ\n","    tsv_files = load_tsv_files_from_folder(folder_path)\n","\n","    for file in tsv_files:\n","        file_path = os.path.join(folder_path, file)\n","        print(f\"íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file}\")\n","\n","        # TSV íŒŒì¼ ì²˜ë¦¬\n","        process_tsv_file(file_path)\n","\n","# ì‹¤í–‰\n","process_files_in_folder(\"C:/python/python(1-0)TSV\")\n"]},{"cell_type":"code","source":["from binance.client import Client\n","import pandas as pd\n","import time\n","\n","# API í‚¤ì™€ Secret í‚¤ ì…ë ¥\n","api_key = \"YOUR_API_KEY\"\n","api_secret = \"YOUR_API_SECRET\"\n","\n","# ë°”ì´ë‚¸ìŠ¤ í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n","client = Client(api_key, api_secret)\n","\n","# ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜\n","def get_historical_15m_data(symbol, start_date, end_date, interval='15m'):\n","    \"\"\"\n","    ë°”ì´ë‚¸ìŠ¤ì—ì„œ ê³¼ê±° 15ë¶„ ë´‰ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ DataFrameìœ¼ë¡œ ë°˜í™˜\n","    :param symbol: ê±°ë˜ í˜ì–´ (ì˜ˆ: \"BTCUSDT\")\n","    :param start_date: ì‹œì‘ ë‚ ì§œ (ì˜ˆ: \"2023-01-01\")\n","    :param end_date: ì¢…ë£Œ ë‚ ì§œ (ì˜ˆ: \"2025-01-31\")\n","    :param interval: ë°ì´í„° ê°„ê²© (ê¸°ë³¸ê°’: '15m')\n","    :return: Pandas DataFrame\n","    \"\"\"\n","    # ë°”ì´ë‚¸ìŠ¤ APIì˜ ì œí•œìœ¼ë¡œ ì¸í•´ ë°ì´í„°ë¥¼ ë‚˜ëˆ ì„œ ìš”ì²­í•´ì•¼ í•¨\n","    all_klines = []\n","    while True:\n","        # ë°ì´í„° ìš”ì²­\n","        klines = client.get_historical_klines(symbol, interval, start_date, end_date)\n","        if not klines:\n","            break\n","\n","        all_klines.extend(klines)\n","\n","        # ë°ì´í„° ëë‚¬ëŠ”ì§€ í™•ì¸\n","        last_timestamp = klines[-1][0]\n","        if last_timestamp >= pd.Timestamp(end_date).timestamp() * 1000:\n","            break\n","\n","        # ë‹¤ìŒ ì‹œì‘ ë‚ ì§œ ì„¤ì •\n","        start_date = pd.Timestamp(last_timestamp, unit='ms') + pd.Timedelta(seconds=1)\n","\n","        # API ìš”ì²­ ì œí•œì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëŒ€ê¸°\n","        time.sleep(0.2)\n","\n","    # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë¦¬\n","    columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume',\n","               'close_time', 'quote_asset_volume', 'number_of_trades',\n","               'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore']\n","\n","    df = pd.DataFrame(all_klines, columns=columns)\n","    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n","    df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]\n","    df[['open', 'high', 'low', 'close', 'volume']] = df[['open', 'high', 'low', 'close', 'volume']].astype(float)\n","\n","    return df\n","\n","# ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n","symbol = \"BTCUSDT\"\n","start_date = \"2023-01-01\"\n","end_date = \"2025-01-31\"\n","interval = \"15m\"\n","\n","print(\"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n","btc_data = get_historical_15m_data(symbol, start_date, end_date, interval)\n","print(\"ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n","\n","# ë°ì´í„° ì €ì¥\n","output_path = r\"C:\\python\\BTC_15m_data.csv\"\n","btc_data.to_csv(output_path, index=False)\n","print(f\"BTC 15ë¶„ ë´‰ ë°ì´í„°ê°€ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"],"metadata":{"id":"kMANtWCHpp0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","input_file = r\"C:\\python\\BTC_15m_data.csv\"\n","output_file = r\"C:\\python\\BTC_15m_data-datetime.csv\"\n","\n","# íŒŒì¼ ì½ê¸° ë° ì²˜ë¦¬\n","if os.path.exists(input_file):\n","    # CSV íŒŒì¼ ì½ê¸°\n","    btc_prices_df = pd.read_csv(input_file)\n","\n","    # 'timestamp' ì—´ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n","    try:\n","        btc_prices_df['timestamp'] = pd.to_datetime(btc_prices_df['timestamp'])\n","        print(\"'timestamp' ì—´ì´ 24ì‹œê°„ì œ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n","        # ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n","        btc_prices_df.to_csv(output_file, index=False)\n","        print(f\"ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}\")\n","    except Exception as e:\n","        print(\"'timestamp' ì—´ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n","        print(\"ì˜¤ë¥˜:\", e)\n","else:\n","    print(f\"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_file}\")\n"],"metadata":{"id":"_AnB-J8npteg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","import requests\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Blockchain.com APIë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¸ëœì­ì…˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n","def get_transaction_inputs_outputs(txid):\n","    url = f\"https://blockchain.info/rawtx/{txid}\"\n","    try:\n","        response = requests.get(url)\n","\n","        # API ì‘ë‹µ ìƒíƒœ ì½”ë“œ í™•ì¸\n","        if response.status_code != 200:\n","            print(f\"Blockchain.com API ìš”ì²­ ì‹¤íŒ¨: ìƒíƒœ ì½”ë“œ {response.status_code}, txid: {txid}\")\n","            return None\n","\n","        data = response.json()\n","\n","        # ì…ë ¥(ì¶œê¸ˆ) ë° ì¶œë ¥(ì…ê¸ˆ) ë°ì´í„° ì²˜ë¦¬\n","        inputs = [\n","            {'address': inp.get('prev_out', {}).get('addr', 'Unknown'),\n","             'value': inp.get('prev_out', {}).get('value', 0) / 1e8,  # Satoshi -> BTC\n","             'type': 'withdrawal'} for inp in data.get('inputs', [])\n","        ]\n","\n","        outputs = [\n","            {'address': out.get('addr', 'Unknown'),\n","             'value': out.get('value', 0) / 1e8,  # Satoshi -> BTC\n","             'type': 'deposit'} for out in data.get('out', [])\n","        ]\n","\n","        # ë°ì´í„° ë³‘í•©\n","        all_data = inputs + outputs\n","        balance_df = pd.DataFrame(all_data)\n","\n","        # íŠ¸ëœì­ì…˜ ë‚ ì§œ ì¶”ê°€\n","        balance_df['date'] = pd.to_datetime(data.get('time', 0), unit='s')\n","\n","        return balance_df\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì˜¤ë¥˜: {e}, txid: {txid}\")\n","        return None\n","    except ValueError as e:\n","        print(f\"JSON ë””ì½”ë“œ ì˜¤ë¥˜: {e}, txid: {txid}\")\n","        return None\n","    except KeyError as e:\n","        print(f\"ë°ì´í„° í‚¤ ì˜¤ë¥˜: {e}, txid: {txid}\")\n","        return None\n","\n","# ë°ì´í„°ë¥¼ CSV íŒŒì¼ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n","def save_to_csv(dataframe, file_path):\n","    \"\"\"ë°ì´í„°ë¥¼ CSV íŒŒì¼ì— ì¶”ê°€ë¡œ ì €ì¥\"\"\"\n","    if not dataframe.empty:\n","        dataframe.to_csv(file_path, mode='a', header=not os.path.exists(file_path), index=False)\n","\n","# ê°œë³„ íŠ¸ëœì­ì…˜ í•´ì‹œë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ ëŒ€ìƒ)\n","def process_transaction(txid, output_file):\n","    try:\n","        balance_df = get_transaction_inputs_outputs(txid)\n","        if balance_df is not None:\n","            save_to_csv(balance_df, output_file)\n","            print(f\"íŠ¸ëœì­ì…˜ '{txid}' ë°ì´í„° ì €ì¥ ì™„ë£Œ.\")\n","        else:\n","            print(f\"íŠ¸ëœì­ì…˜ '{txid}' ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n","            return (txid, \"Unprocessed\")\n","    except Exception as e:\n","        print(f\"íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}, txid: {txid}\")\n","        return (txid, \"Failed\")\n","\n","    return (txid, \"Processed\")\n","\n","# íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n","def process_file(input_file, output_file):\n","    error_file = output_file.replace(\".csv\", \"_errors.csv\")\n","\n","    try:\n","        # íŒŒì¼ ì½ê¸°\n","        df = pd.read_csv(input_file)\n","\n","        # 'hash' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸\n","        if 'hash' not in df.columns:\n","            print(f\"íŒŒì¼ '{input_file}'ì— 'hash' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n","            return\n","\n","        txids = df['hash'].tolist()\n","        print(f\"íŒŒì¼ '{input_file}'ì—ì„œ íŠ¸ëœì­ì…˜ í•´ì‹œ ì¶”ì¶œ ì™„ë£Œ.\")\n","\n","        # ë³‘ë ¬ ì²˜ë¦¬\n","        results = []\n","        with ThreadPoolExecutor(max_workers=8) as executor:  # ë³‘ë ¬ ì‘ì—… ìŠ¤ë ˆë“œ ìˆ˜ ì¡°ì •\n","            futures = {executor.submit(process_transaction, txid, output_file): txid for txid in txids}\n","            for future in futures:\n","                txid = futures[future]\n","                try:\n","                    result = future.result()\n","                    if result:\n","                        results.append(result)\n","                except Exception as e:\n","                    print(f\"íŠ¸ëœì­ì…˜ '{txid}' ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n","                    results.append((txid, \"Failed\"))\n","\n","        # ì˜¤ë¥˜ íŠ¸ëœì­ì…˜ ì €ì¥\n","        error_data = [res for res in results if res[1] != \"Processed\"]\n","        if error_data:\n","            error_df = pd.DataFrame(error_data, columns=[\"Txid\", \"Status\"])\n","            error_df.to_csv(error_file, index=False)\n","            print(f\"ì˜¤ë¥˜ ë°œìƒ íŠ¸ëœì­ì…˜ ì €ì¥ ì™„ë£Œ: {error_file}\")\n","\n","        print(f\"ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ. ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}\")\n","\n","    except Exception as e:\n","        print(f\"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}, íŒŒì¼: {input_file}\")\n","\n","# ì‹¤í–‰\n","input_file = \"C:/python/(1-1)ëª¨ë“  transactions.csv\"\n","output_file = \"C:/python/(2-0)ë³‘ë ¬ì²˜ë¦¬.csv\"\n","process_file(input_file, output_file)\n"],"metadata":{"id":"szwrpLaypthH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# ì›ë³¸ íŒŒì¼ê³¼ ìƒˆë¡œìš´ íŒŒì¼ ê²½ë¡œ\n","source_file = \"C:/python/(2-0)ë³‘ë ¬ì²˜ë¦¬.csv\"\n","new_file = \"C:/python/(2-1-0)ì†Œí˜• ë°ì´í„° ì œê±°.csv\"\n","\n","# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜\n","def process_file(source_file, new_file):\n","    if not os.path.exists(source_file):\n","        print(f\"ì›ë³¸ íŒŒì¼ '{source_file}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    try:\n","        # CSV íŒŒì¼ ì½ê¸° (êµ¬ë¶„ì ëª…ì‹œ + ë¬¸ì œ ìˆëŠ” í–‰ ìŠ¤í‚µ)\n","        df = pd.read_csv(source_file, sep=\",\", low_memory=False, on_bad_lines=\"skip\")\n","\n","        # 'value' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸\n","        if 'value' not in df.columns:\n","            print(\"'value' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n","            return\n","\n","        # 'value' ì»¬ëŸ¼ì„ ìˆ«ìë¡œ ë³€í™˜, ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê°’ì€ NaNìœ¼ë¡œ ì²˜ë¦¬\n","        df['value'] = pd.to_numeric(df['value'], errors='coerce')\n","\n","        # 'value' ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ì´ 10 ì´ìƒì¸ í–‰ë§Œ í•„í„°ë§\n","        filtered_df = df[df['value'] > 10]\n","\n","        # ìƒˆë¡œìš´ íŒŒì¼ì— ì €ì¥\n","        filtered_df.to_csv(new_file, index=False)\n","        print(f\"íŒŒì¼ '{new_file}'ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","    except Exception as e:\n","        print(f\"íŒŒì¼ '{source_file}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","\n","# ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜\n","def run():\n","    with ThreadPoolExecutor() as executor:\n","        executor.submit(process_file, source_file, new_file)\n","\n","# ì‹¤í–‰\n","run()\n","\n","print(\"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!\")\n"],"metadata":{"id":"nTLCYHYQptjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","input_file = 'C:/python/(2-1-0)ì†Œí˜• ë°ì´í„° ì œê±°.csv'\n","output_file = 'C:/python/(2-1-1)address ê¸°ì¤€ ì •ë ¬.csv'\n","\n","# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜\n","def process_file(input_file, output_file):\n","    if not os.path.exists(input_file):\n","        print(f\"ì›ë³¸ íŒŒì¼ '{input_file}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n","        return\n","\n","    try:\n","        # CSV íŒŒì¼ ì½ê¸°\n","        df = pd.read_csv(input_file)\n","\n","        # 'address' ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ 1ìˆœìœ„, 'date' ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ 2ìˆœìœ„ë¡œ ì •ë ¬\n","        df_sorted = df.sort_values(by=['address', 'date'])\n","\n","        # ì •ë ¬ëœ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n","        df_sorted.to_csv(output_file, index=False)\n","        print(f\"íŒŒì¼ì´ '{output_file}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","    except Exception as e:\n","        print(f\"íŒŒì¼ '{input_file}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","\n","# ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜\n","def run():\n","    with ThreadPoolExecutor() as executor:\n","        executor.submit(process_file, input_file, output_file)\n","\n","# ì‹¤í–‰\n","run()\n","\n","print(\"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!\")\n"],"metadata":{"id":"tBMnnKzvptmW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import concurrent.futures\n","import re\n","\n","# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","input_file = r\"C:/python/(2-1-1)address ê¸°ì¤€ ì •ë ¬.csv\"\n","\n","# 'date' í˜•ì‹ì„ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n","def format_date(value):\n","    value = str(value)\n","    # ì •ê·œ í‘œí˜„ì‹ ì ìš©í•˜ì—¬ HH:00:00 í˜•íƒœë¡œ ë³€í™˜\n","    value = re.sub(r\"^(\\d{4}-\\d{2}-\\d{2} \\d{2})$\", r\"\\1:00:00\", value)\n","    formatted_value = pd.to_datetime(value, errors='coerce')\n","    return formatted_value if not pd.isna(formatted_value) else value\n","\n","# íŒŒì¼ ë‚´ 'date' ì—´ ë³€í™˜ í•¨ìˆ˜\n","def transform_date_column(input_file):\n","    print(\"ğŸ“¥ ë°ì´í„° ë¡œë”© ì¤‘...\")\n","    df = pd.read_csv(input_file, dtype={'date': 'string'})  # date ì—´ì„ ë¬¸ìì—´ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°\n","\n","    print(\"ğŸš€ 'date' ë³€í™˜ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)\")\n","    with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:\n","        df['date'] = list(executor.map(format_date, df['date'], chunksize=10000))\n","\n","    print(\"ğŸ’¾ ë³€í™˜ ì™„ë£Œ! íŒŒì¼ ì €ì¥ ì¤‘...\")\n","    df.to_csv(input_file, index=False)  # ë®ì–´ì“°ê¸°\n","\n","    print(f\"âœ… 'date' ë³€í™˜ ì™„ë£Œ ë° ì €ì¥ ì™„ë£Œ: {input_file}\")\n","\n","# âœ… Windowsì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œ í•„ìˆ˜ì ì¸ êµ¬ì¡°\n","if __name__ == '__main__':\n","    transform_date_column(input_file)\n"],"metadata":{"id":"sVKUBU43pto6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","input_file = r\"C:/python/(2-1-1)address ê¸°ì¤€ ì •ë ¬.csv\"\n","btc_prices_file = r\"C:\\python\\BTC_15m_data-datetime.csv\"\n","\n","# BTC ê°€ê²© ë°ì´í„° ë¡œë“œ\n","btc_prices_df = pd.read_csv(btc_prices_file)\n","btc_prices_df['timestamp'] = pd.to_datetime(btc_prices_df['timestamp']).dt.floor('15min')\n","\n","def update_price_in_file(input_file):\n","    # í•„í„°ë§ëœ ë°ì´í„° íŒŒì¼ ë¡œë“œ\n","    filtered_df = pd.read_csv(input_file)\n","    filtered_df['date'] = pd.to_datetime(filtered_df['date']).dt.floor('15min')  # 15ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜\n","\n","    # ğŸ”¥ Merge í™œìš© (ë¹ ë¦„)\n","    merged_df = filtered_df.merge(btc_prices_df[['timestamp', 'open']],\n","                                  left_on='date', right_on='timestamp',\n","                                  how='left')\n","\n","    # 'price' ì—´ ì—…ë°ì´íŠ¸\n","    merged_df.rename(columns={'open': 'price'}, inplace=True)\n","    merged_df.drop(columns=['timestamp'], inplace=True)\n","\n","    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ì €ì¥\n","    output_file_path = input_file.replace(\".csv\", \"_updated.csv\")\n","    merged_df.to_csv(output_file_path, index=False)\n","    print(f\"âœ… 'price' ì—´ì´ 15ë¶„ ë‹¨ìœ„ë¡œ ì—…ë°ì´íŠ¸ë˜ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file_path}\")\n","\n","# ì‹¤í–‰\n","update_price_in_file(input_file)\n"],"metadata":{"id":"nZvKnK8DptrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# CSV íŒŒì¼ ì½ê¸° (date ì»¬ëŸ¼ì„ ë‚ ì§œí˜•ìœ¼ë¡œ íŒŒì‹±)\n","file_path = \"C:/python/(2-1-1)address ê¸°ì¤€ ì •ë ¬_updated.csv\"\n","df = pd.read_csv(file_path, parse_dates=['date'])\n","\n","# ì¡°ê±´: \"withdrawal â†’ deposit\" ë˜ëŠ” \"deposit â†’ withdrawal\"\n","cond_type = ((df['type'] == 'withdrawal') & (df['type'].shift(-1) == 'deposit')) | \\\n","            ((df['type'] == 'deposit') & (df['type'].shift(-1) == 'withdrawal'))\n","\n","# ë‚ ì§œ ì°¨ì´ 2ë¶„ ì´í•˜\n","cond_date = (df['date'].shift(-1) - df['date']).abs() <= pd.Timedelta(minutes=2)\n","\n","# ê°€ê²© ë™ì¼\n","cond_price = df['price'] == df['price'].shift(-1)\n","\n","# ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°, í•´ë‹¹ í–‰ê³¼ ë‹¤ìŒ í–‰ ëª¨ë‘ ì‚­ì œ\n","mask = cond_type & cond_date & cond_price\n","idx = df.index[mask]\n","indices_to_drop = idx.union(idx + 1)\n","\n","df_filtered = df.drop(indices_to_drop).reset_index(drop=True)\n","\n","# ê²°ê³¼ ì €ì¥\n","df_filtered.to_csv(\"C:/python/(2-1-1)address ê¸°ì¤€ ì •ë ¬_updated_filtered.csv\", index=False)\n","\n","print(\"ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” í–‰ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"],"metadata":{"id":"J0U832mWptt9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from datetime import timedelta\n","from concurrent.futures import ProcessPoolExecutor\n","\n","def process_address_group(group):\n","    local_to_delete = set()  # setìœ¼ë¡œ ì¤‘ë³µì„ ë°©ì§€\n","    group_sorted = group.sort_values(by='date')  # ê±°ë˜ì¼ ê¸°ì¤€ ì •ë ¬\n","\n","    total_transactions = len(group_sorted)  # ì „ì²´ ê±°ë˜ ìˆ˜\n","\n","    # 30ì¼ ë‚´ ê±°ë˜ë¥¼ ë¯¸ë¦¬ ê³„ì‚° (ì¤‘ë³µ ë°©ì§€)\n","    date_range_dict = {}\n","    for i in range(len(group_sorted)):\n","        current_time = group_sorted.iloc[i]['date']\n","        # 30ì¼ ë‚´ ê±°ë˜ë¥¼ ì´ë¯¸ ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ ì €ì¥\n","        time_window = group_sorted[(group_sorted['date'] >= current_time - timedelta(days=30)) &\n","                                   (group_sorted['date'] <= current_time)]\n","        date_range_dict[current_time] = len(time_window)\n","\n","    # 30ì¼ ë²”ìœ„ ë‚´ì˜ ê±°ë˜ ìˆ˜ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ ê³„ì‚°ì„ ë°©ì§€\n","    for i in range(len(group_sorted)):\n","        current_time = group_sorted.iloc[i]['date']\n","        count_in_window = date_range_dict[current_time]  # ë¯¸ë¦¬ ê³„ì‚°ëœ ê°’ ì‚¬ìš©\n","\n","        # 70%ê°€ 30ì¼ ë‚´ì— ëª°ë ¤ìˆìœ¼ë©´ í•´ë‹¹ í–‰ ì‚­ì œ\n","        if count_in_window / total_transactions >= 0.7:\n","            local_to_delete.add(group_sorted.index[i])  # setì— ì¶”ê°€í•˜ì—¬ ì¤‘ë³µ ì œê±°\n","\n","    return local_to_delete\n","\n","def main():\n","    # CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n","    file_path = \"C:/python/(2-1-1)address ê¸°ì¤€ ì •ë ¬_updated_filtered.csv\"\n","    df = pd.read_csv(file_path)\n","\n","    # 'date' ì»¬ëŸ¼ì„ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì´ë¯¸ datetime í˜•ì‹ì´ë¼ë©´ ì´ ê³¼ì •ì€ ìƒëµ ê°€ëŠ¥í•©ë‹ˆë‹¤)\n","    df['date'] = pd.to_datetime(df['date'])\n","\n","    # ì‚­ì œí•  í–‰ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n","    to_delete = set()\n","\n","    # ProcessPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬\n","    with ProcessPoolExecutor() as executor:\n","        # 'address'ë³„ë¡œ ê·¸ë£¹í™”í•œ ë’¤ ë³‘ë ¬ ì²˜ë¦¬\n","        results = list(executor.map(process_address_group, [group for _, group in df.groupby('address')]))\n","\n","    # ê²°ê³¼ë¥¼ í•©ì¹¨\n","    for result in results:\n","        to_delete.update(result)\n","\n","    # ì‚­ì œí•  í–‰ ì œê±°\n","    df_cleaned = df.drop(index=to_delete)\n","\n","    # ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n","    df_cleaned.to_csv(\"C:/python/(2-2-2)cleaned.csv\", index=False)\n","\n","# ë©”ì¸ ëª¨ë“ˆë¡œ ì‹¤í–‰ë  ë•Œë§Œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"xh0YGeRdp3e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import multiprocessing as mp\n","\n","# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","input_file = r'C:/python/(2-2-2)cleaned.csv'\n","output_file = r'C:/python/(3-0-0) type_num ë¶€ì—¬ + aver_price êµ¬í•˜ê¸°.csv'\n","\n","def assign_type_num(group):\n","    \"\"\"type_num í• ë‹¹\"\"\"\n","    group = group.copy().reset_index(drop=True)\n","\n","    # ğŸ”¹ type_num í• ë‹¹ (type ë³€ê²½ ì‹œë§ˆë‹¤ ì¦ê°€)\n","    group['type_change'] = (group['type'] != group['type'].shift()).astype(int)\n","    group['type_num'] = group['type_change'].cumsum()\n","    group.drop(columns=['type_change'], inplace=True)\n","\n","    # ğŸ”¹ aver_price ê³„ì‚° (type_num ë³„ í‰ê·  ê³„ì‚°)\n","    group['aver_price'] = group.groupby('type_num')['price'].transform('mean')\n","\n","    return group\n","\n","def process_data_parallel(df, func):\n","    \"\"\"ê° addressë³„ ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬\"\"\"\n","    with mp.Pool(processes=mp.cpu_count()) as pool:\n","        results = pool.map(func, [group for _, group in df.groupby('address')])\n","    return pd.concat(results)\n","\n","if __name__ == '__main__':\n","    # CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n","    df = pd.read_csv(input_file)\n","\n","    # addressì™€ dateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n","    df = df.sort_values(by=[\"address\", \"date\"])\n","\n","    # ğŸ”¹ ë³‘ë ¬ ì²˜ë¦¬ë¡œ type_numê³¼ aver_price ê³„ì‚°\n","    df = process_data_parallel(df, assign_type_num)\n","\n","    # ê²°ê³¼ CSV ì €ì¥\n","    df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n","\n","    print(f\"âœ… ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}\")\n"],"metadata":{"id":"kQZcKQgjp3cT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","\n","# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","input_file = 'C:/python/(3-0-0) type_num ë¶€ì—¬ + aver_price êµ¬í•˜ê¸°.csv'\n","output_file = 'C:/python/(3-0-1) type_num ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©.csv'\n","\n","# íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n","if os.path.exists(input_file):\n","    # CSV íŒŒì¼ ë¡œë“œ\n","    df = pd.read_csv(input_file)\n","\n","    # addressì™€ type_numì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n","    df_sorted = df.sort_values(by=['address', 'type_num'])\n","\n","    # addressë³„ë¡œ ë™ì¼í•œ type_num í–‰ì„ ë³‘í•©\n","    # ì—¬ê¸°ì„œ type_numì„ ê·¸ëƒ¥ ì²«ë²ˆì§¸ ê°’ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •\n","    df_merged = df_sorted.groupby(['address', 'type_num'], as_index=False).agg(\n","        {'value': 'sum',\n","         'type': 'first',\n","         'price': 'first',\n","         'aver_price': 'first'}).reset_index(drop=True)\n","\n","    # ë³‘í•©ëœ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n","    if not os.path.exists(output_file):\n","        df_merged.to_csv(output_file, index=False)\n","    else:\n","        df_merged.to_csv(output_file, index=False)\n","\n","    print(\"ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","else:\n","    print(\"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n"],"metadata":{"id":"wGfBWiafp3Zz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import os\n","from concurrent.futures import ProcessPoolExecutor\n","\n","# LONG, SHORT êµ¬ë¶„ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜\n","def process_address(address_group):\n","    address, group = address_group\n","    # ì²« ë²ˆì§¸ í–‰ì´ withdrawalì´ë©´ í•´ë‹¹ addressëŠ” ë¬´ì‹œ\n","    if group.iloc[0]['type'] == 'withdrawal':\n","        return None\n","\n","    # depositê³¼ withdrawalì„ êµ¬ë¶„\n","    deposit_data = group[group['type'] == 'deposit']\n","    withdrawal_data = group[group['type'] == 'withdrawal']\n","\n","    if not deposit_data.empty and not withdrawal_data.empty:\n","        # depositì˜ aver_priceì™€ withdrawalì˜ aver_price ë¹„êµ\n","        deposit_aver_price = deposit_data['aver_price'].iloc[0]\n","        withdrawal_aver_price = withdrawal_data['aver_price'].iloc[0]\n","\n","        # LONGê³¼ SHORT ê³„ì‚°\n","        if deposit_aver_price < withdrawal_aver_price:\n","            result = 'LONG'\n","        else:\n","            result = 'SHORT'\n","\n","        return {'address': address, 'result': result}\n","\n","def main():\n","    # íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","    input_file = 'C:/python/(3-0-1) type_num ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©.csv'\n","    output_file = 'C:/python/(3-0-2) (ìŠ¹ë¥ ) address ë¡± ìˆ ë¶„ë¥˜.csv'\n","\n","    # CSV íŒŒì¼ ë¡œë“œ\n","    df = pd.read_csv(input_file)\n","\n","    # addressë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”\n","    address_groups = df.groupby('address')\n","\n","    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê° address ì²˜ë¦¬\n","    with ProcessPoolExecutor() as executor:\n","        results = list(executor.map(process_address, address_groups))\n","\n","    # Noneì„ ì œì™¸í•œ ê²°ê³¼ë§Œ í•„í„°ë§\n","    results = [result for result in results if result is not None]\n","\n","    # ê²°ê³¼ DataFrame ìƒì„±\n","    result_df = pd.DataFrame(results)\n","\n","    # addressë³„ë¡œ LONG, SHORT ë¹„ìœ¨ ê³„ì‚°\n","    final_results = []\n","\n","    for address, group in result_df.groupby('address'):\n","        long_count = (group['result'] == 'LONG').sum()\n","        short_count = (group['result'] == 'SHORT').sum()\n","\n","        # LONG:SHORT ë¹„ìœ¨ì´ 6:4 ì´ìƒì´ë©´ LONG, ë°˜ëŒ€ë©´ SHORT\n","        if long_count / (long_count + short_count) >= 0.6:\n","            final_results.append({'address': address, 'prediction': 'LONG'})\n","        else:\n","            final_results.append({'address': address, 'prediction': 'SHORT'})\n","\n","    # ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±\n","    final_df = pd.DataFrame(final_results)\n","\n","    # íŒŒì¼ë¡œ ì €ì¥\n","    final_df.to_csv(output_file, index=False)\n","\n","    print(\"ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n","\n","# ì´ ì½”ë“œê°€ ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ main()ì„ í˜¸ì¶œ\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"T-rG9Z7Pp3XH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from concurrent.futures import ProcessPoolExecutor\n","\n","# CSV íŒŒì¼ ê²½ë¡œ\n","file_path = 'C:/python/(2-2-2)cleaned.csv'\n","output_path = 'C:/python/(3-1-1) (ìˆœìˆ˜ìµ) long short ë¶„ë¥˜.csv'\n","\n","# ë°ì´í„° ë¡œë“œ\n","df = pd.read_csv(file_path)\n","\n","# depositê³¼ withdrawal ë³„ë¡œ value * price ê³„ì‚°\n","df['amount'] = df['value'] * df['price']\n","\n","def classify_long_short(group):\n","    a = group.loc[group['type'] == 'deposit', 'amount'].sum()\n","    b = group.loc[group['type'] == 'withdrawal', 'amount'].sum()\n","\n","    long_short_type = 'LONG' if a < b else 'SHORT'\n","\n","\n","    return long_short_type\n","\n","def process_address_group(address_group):\n","    address, group = address_group\n","    long_short_type = classify_long_short(group)\n","    return address, long_short_type  # return a tuple\n","\n","# ê° addressë³„ LONG_SHORT_TYPE ê³„ì‚° (ë³‘ë ¬ ì²˜ë¦¬)\n","def main():\n","    with ProcessPoolExecutor() as executor:\n","        results = list(executor.map(process_address_group, df.groupby('address')))\n","\n","    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n","    result_df = pd.DataFrame(results, columns=['address', 'LONG_SHORT_TYPE'])\n","\n","    # ê²°ê³¼ ì €ì¥\n","    result_df.to_csv(output_path, index=False)\n","\n","    print(f\"ê²°ê³¼ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}\")\n","\n","# ë©”ì¸ ëª¨ë“ˆë¡œ ì‹¤í–‰ë  ë•Œë§Œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘\n","if __name__ == '__main__':\n","    main()\n"],"metadata":{"id":"S-KvPXXcp3Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# ì…ë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","input_file = 'C:/python/(3-0-0) type_num ë¶€ì—¬ + aver_price êµ¬í•˜ê¸°.csv'  # ë‹¨ì¼ íŒŒì¼ ê²½ë¡œ\n","\n","# ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ ì„¤ì • (í•˜ë‚˜ì˜ íŒŒì¼ì— ì €ì¥)\n","output_file_path = 'C:/python/(3-2)whale_type.csv'\n","\n","# ì²« ë²ˆì§¸ íŒŒì¼ì—ì„œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (í—¤ë” í¬í•¨)\n","is_first_file = not os.path.exists(output_file_path)\n","\n","try:\n","    # íŒŒì¼ì„ ì½ì–´ DataFrame ìƒì„±\n","    df = pd.read_csv(input_file)\n","\n","    # addressë³„ë¡œ valueì˜ í‰ê·  ê³„ì‚°\n","    address_value_avg = df.groupby('address')['value'].mean()\n","\n","    # whale_type ë¶„ë¥˜\n","    whale_type = []\n","    for avg_value in address_value_avg:\n","        if 10 <= avg_value < 100:\n","            whale_type.append('small')\n","        elif 100 <= avg_value < 500:\n","            whale_type.append('medium')\n","        else:\n","            whale_type.append('large')\n","\n","    # ìƒˆë¡œìš´ DataFrame ìƒì„±: addressì™€ whale_type\n","    whale_df = pd.DataFrame({\n","        'address': address_value_avg.index,\n","        'whale_type': whale_type\n","    })\n","\n","    # íŒŒì¼ì— ë°ì´í„°ë¥¼ ì¶”ê°€ (ì²« ë²ˆì§¸ íŒŒì¼ì´ë©´ í—¤ë” í¬í•¨)\n","    whale_df.to_csv(output_file_path, mode='a', header=is_first_file, index=False)\n","    print(f\"íŒŒì¼ '{input_file}' ì²˜ë¦¬ ì™„ë£Œ!\")\n","\n","except Exception as e:\n","    print(f\"íŒŒì¼ '{input_file}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","\n","print(\"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!\")\n"],"metadata":{"id":"9JGp4yqzp3R-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# ì…ë ¥ íŒŒì¼ ê²½ë¡œ (ë‹¨ì¼ íŒŒì¼)\n","input_file = \"C:/python/(3-0-0) type_num ë¶€ì—¬ + aver_price êµ¬í•˜ê¸°.csv\"\n","result_file_path = \"C:/python/(3-3)trader_type_analysis.csv\"\n","\n","# ê±°ë˜ ì£¼ê¸° ë¶„ë¥˜ í•¨ìˆ˜\n","def analyze_trader_type(df):\n","    # í™€ìˆ˜ (ì‹œì‘ ê±°ë˜)ì™€ ì§ìˆ˜ (ë ê±°ë˜) ë‚ ì§œ ì¶”ì¶œ\n","    odd_dates = df[df['type_num'] % 2 == 1]['date']\n","    even_dates = df[df['type_num'] % 2 == 0]['date']\n","\n","    # ë‚ ì§œ ì°¨ì´ ê³„ì‚°\n","    date_diffs = []\n","    min_length = min(len(odd_dates), len(even_dates))  # í™€ìˆ˜ì™€ ì§ìˆ˜ ë‚ ì§œ ì¤‘ ë” ì‘ì€ ê¸¸ì´\n","\n","    for i in range(min_length):\n","        try:\n","            # í™€ìˆ˜ ë‚ ì§œì™€ ì§ìˆ˜ ë‚ ì§œ ì°¨ì´ ê³„ì‚° (ì¼ ë‹¨ìœ„)\n","            date_diff = (even_dates.iloc[i] - odd_dates.iloc[i]).days\n","            date_diffs.append(date_diff)\n","        except IndexError:\n","            continue  # ì¸ë±ìŠ¤ ì˜¤ë¥˜ê°€ ë‚˜ë©´ ë„˜ì–´ê°€ê¸°\n","\n","    # ë‚ ì§œ ì°¨ì´ì˜ í‰ê·  ê³„ì‚°\n","    avg_trade_duration = sum(date_diffs) / len(date_diffs) if date_diffs else 0\n","\n","    # ê±°ë˜ ì£¼ê¸° ë¶„ë¥˜\n","    if avg_trade_duration <= 1:\n","        trader_type = 'scalper'\n","    elif 1 < avg_trade_duration <= 14:\n","        trader_type = 'short swing'\n","    elif 15 <= avg_trade_duration <= 30:\n","        trader_type = 'long swing'\n","    else:\n","        trader_type = 'long term'\n","\n","    return trader_type\n","\n","# ê°œë³„ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜\n","def process_file(file_path):\n","    df = pd.read_csv(file_path)\n","    df['date'] = pd.to_datetime(df['date']).dt.date\n","\n","    result_list = []\n","\n","    for address, group in df.groupby('address'):  # ì£¼ì†Œë³„ ê·¸ë£¹í™”\n","        trader_type = analyze_trader_type(group)  # ê°œë³„ addressì˜ ê±°ë˜ ì£¼ê¸° ë¶„ì„\n","        result_list.append({'address': address, 'trader_type': trader_type})  # ê²°ê³¼ ì €ì¥\n","\n","    result_df = pd.DataFrame(result_list)  # ìµœì¢… ë°ì´í„°í”„ë ˆì„ ìƒì„±\n","    return result_df\n","\n","\n","# ê²°ê³¼ íŒŒì¼ ìƒì„± í•¨ìˆ˜\n","def create_trader_type_file():\n","    # ë‹¨ì¼ íŒŒì¼ì„ ì²˜ë¦¬\n","    result_df = process_file(input_file)\n","\n","    # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥\n","    result_df.to_csv(result_file_path, index=False)\n","    print(f\"âœ… íŠ¸ë ˆì´ë” ìœ í˜• ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼: {result_file_path}\")\n","\n","# ì‹¤í–‰\n","if __name__ == \"__main__\":\n","    create_trader_type_file()\n"],"metadata":{"id":"USC9uzybp3PY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# íŒŒì¼ ê²½ë¡œ\n","input_file_path = \"C:/python/(3-3)trader_type_analysis.csv\"\n","output_file_path = \"C:/python/(3-4)trader_type_analysis_most.csv\"\n","\n","# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n","df = pd.read_csv(input_file_path)\n","\n","# addressë³„ë¡œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ trader_typeì„ ì¶”ì¶œ\n","most_common_trader_type = df.groupby('address')['trader_type'].agg(lambda x: x.mode()[0]).reset_index()\n","\n","# ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ CSVë¡œ ì €ì¥\n","most_common_trader_type.to_csv(output_file_path, index=False)\n","\n","print(f\"âœ… addressë³„ë¡œ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ trader_typeì´ ì¶”ì¶œëœ ê²°ê³¼ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file_path}\")\n"],"metadata":{"id":"S8OQ0BIlp3M2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# íŒŒì¼ ê²½ë¡œ ìˆ˜ì • (C:\\python ê²½ë¡œì— ìˆëŠ” íŒŒì¼ë“¤)\n","long_short_file_1_path = \"C:/python/(3-0-2) (ìŠ¹ë¥ ) address ë¡± ìˆ ë¶„ë¥˜.csv\"\n","long_short_file_2_path = \"C:/python/(3-1-1) (ìˆœìˆ˜ìµ) long short ë¶„ë¥˜.csv\"\n","trader_type_file_path = \"C:/python/(3-4)trader_type_analysis_most.csv\"\n","whale_type_file_path = \"C:/python/(3-2)whale_type.csv\"\n","\n","# ê° íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê¸°\n","long_short_1_df = pd.read_csv(long_short_file_1_path)\n","long_short_2_df = pd.read_csv(long_short_file_2_path)\n","trader_type_df = pd.read_csv(trader_type_file_path)\n","whale_type_df = pd.read_csv(whale_type_file_path)\n","\n","# ê° íŒŒì¼ì„ address ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©\n","merged_df = pd.merge(long_short_1_df, long_short_2_df, on='address', how='outer')\n","merged_df = pd.merge(merged_df, trader_type_df, on='address', how='outer')\n","merged_df = pd.merge(merged_df, whale_type_df, on='address', how='outer')\n","\n","# '|'a-b|' ì»¬ëŸ¼ ì‚­ì œ\n","if '|a-b|' in merged_df.columns:\n","    merged_df.drop(columns='|a-b|', inplace=True)\n","\n","# ê²°ê³¼ ì¶œë ¥ (ì˜µì…˜)\n","print(merged_df.head())\n","\n","# ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n","merged_df.to_csv('C:/python/(3-5)ê°ê°ì˜ ê²°ê³¼íŒŒì¼.csv', index=False)\n"],"metadata":{"id":"vN5kodU3p3KJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# ê¸°ì¡´ íŒŒì¼ ê²½ë¡œ\n","merged_file_path = \"C:/python/(3-5)ê°ê°ì˜ ê²°ê³¼íŒŒì¼.csv\"\n","output_file_path = \"C:/python/(3-6)prediction, LONG_SHORT_TYPE ì¼ì¹˜.csv\"\n","\n","# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n","merged_df = pd.read_csv(merged_file_path)\n","\n","# predictionê³¼ LONG_SHORT_TYPEì´ ì¼ì¹˜í•˜ëŠ” addressë§Œ í•„í„°ë§\n","filtered_df = merged_df[merged_df['prediction'] == merged_df['LONG_SHORT_TYPE']]\n","\n","# í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œ\n","final_df = filtered_df[['address', 'prediction', 'trader_type', 'whale_type']]\n","\n","# ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ íŒŒì¼ë¡œ ì €ì¥\n","final_df.to_csv(output_file_path, index=False)\n","\n","print(f\"âœ… í•„í„°ë§ëœ ê²°ê³¼ê°€ '{output_file_path}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n"],"metadata":{"id":"8cUfBdaYp3Gz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import aiohttp\n","import asyncio\n","from datetime import datetime\n","\n","# API ì •ë³´\n","BITCORE_API_URL = \"https://api.bitcore.io/api/BTC/mainnet/address/\"\n","DATA_FILE = \"C:/python/(3-6)prediction, LONG_SHORT_TYPE ì¼ì¹˜.csv\"\n","OUTPUT_FILE = \"C:/python/(4-0) ìµœê·¼ ë¸”ë¡ ì¡°íšŒ.csv\"\n","MAX_CONCURRENT_REQUESTS = 10  # ë™ì‹œ ìš”ì²­ ê°œìˆ˜\n","MAX_ROWS = 31296  # ìµœëŒ€ ì²˜ë¦¬ í–‰ ìˆ˜\n","\n","# API ìš”ì²­ í•¨ìˆ˜ (ì¬ì‹œë„ íšŸìˆ˜ ì œí•œ)\n","async def fetch_transaction(session, address, retries=3):\n","    url = BITCORE_API_URL + address + \"/txs\"\n","    try:\n","        async with session.get(url) as response:\n","            if response.status == 200:\n","                transactions = await response.json()\n","                if transactions:\n","                    latest_tx = transactions[0]\n","                    is_deposit = any(address in vin.get(\"addr\", \"\") for vin in latest_tx.get(\"vin\", []))\n","                    print(f\"Success: {address} - {('deposit' if is_deposit else 'withdrawal')}\")\n","                    return {\"address\": address, \"transaction_type\": \"deposit\" if is_deposit else \"withdrawal\"}\n","\n","            elif response.status == 429 and retries > 0:\n","                print(f\"Rate Limit Exceeded for {address}. Retrying in 5 seconds... ({retries} retries left)\")\n","                await asyncio.sleep(5)\n","                return await fetch_transaction(session, address, retries - 1)\n","\n","            else:\n","                print(f\"Failed: {address} - Status code {response.status}\")\n","    except Exception as e:\n","        print(f\"Error: {address} - {str(e)}\")\n","\n","    return {\"address\": address, \"transaction_type\": None}\n","\n","# ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n","async def fetch_all_transactions(addresses):\n","    address_data = []\n","    async with aiohttp.ClientSession() as session:\n","        tasks = []\n","        for address in addresses:\n","            tasks.append(fetch_transaction(session, address))\n","            if len(tasks) >= MAX_CONCURRENT_REQUESTS:\n","                results = await asyncio.gather(*tasks)\n","                update_results(results)\n","                tasks = []\n","                if check_row_limit():\n","                    return\n","        if tasks:\n","            results = await asyncio.gather(*tasks)\n","            update_results(results)\n","\n","# ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ í•¨ìˆ˜\n","def update_results(new_data):\n","    new_df = pd.DataFrame(new_data)\n","    if os.path.exists(OUTPUT_FILE):\n","        try:\n","            df_output = pd.read_csv(OUTPUT_FILE)\n","        except pd.errors.EmptyDataError:\n","            df_output = pd.DataFrame(columns=[\"address\", \"transaction_type\"])\n","    else:\n","        df_output = pd.DataFrame(columns=[\"address\", \"transaction_type\"])\n","\n","    if not new_df.empty and \"address\" in new_df.columns:\n","        df_output = df_output.set_index(\"address\")\n","        new_df = new_df.set_index(\"address\")\n","        df_output.update(new_df)\n","        df_output = df_output.reset_index()\n","\n","    df_output.to_csv(OUTPUT_FILE, index=False)\n","    print(f\"Updated {len(new_df)} records. Total: {len(df_output)}\")\n","\n","# í˜„ì¬ í–‰ ìˆ˜ í™•ì¸ í›„ ìµœëŒ€ê°’ ì´ˆê³¼ ì‹œ ì¤‘ë‹¨\n","def check_row_limit():\n","    if os.path.exists(OUTPUT_FILE):\n","        df_output = pd.read_csv(OUTPUT_FILE)\n","        if len(df_output) >= MAX_ROWS:\n","            print(f\"Reached max rows ({MAX_ROWS}). Stopping process.\")\n","            return True\n","    return False\n","\n","# ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°\n","df = pd.read_csv(DATA_FILE)\n","addresses = set(df['address'].dropna().unique()) if 'address' in df.columns else set()\n","print(f\"Total addresses to process: {len(addresses)}\")\n","\n","# ì‹¤í–‰ ì‹œì‘ ì‹œê°„ ê¸°ë¡\n","start_time = datetime.now()\n","print(f\"Process started at {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","\n","# ë¹„ë™ê¸° ì‹¤í–‰\n","asyncio.run(fetch_all_transactions(addresses))\n","\n","# ì‹¤í–‰ ì™„ë£Œ ì‹œê°„ ê¸°ë¡\n","end_time = datetime.now()\n","elapsed_time = end_time - start_time\n","hours, remainder = divmod(elapsed_time.total_seconds(), 3600)\n","minutes, seconds = divmod(remainder, 60)\n","formatted_time = f\"{int(hours)}h {int(minutes)}m {int(seconds)}s\"\n","\n","# ì¢…ë£Œ ì‹œê°„ ì¶œë ¥\n","print(f\"Process completed at {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n","print(f\"Total time taken: {formatted_time}\")\n"],"metadata":{"id":"HZ4-8rR5qEdn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import pandas as pd\n","import requests\n","import concurrent.futures\n","from datetime import datetime\n","\n","# í´ë” ë° íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","transactions_folder = \"C:/python/python(1-0)TSV\"\n","recent_blocks_file = \"C:/python/(4-0) ìµœê·¼ ë¸”ë¡ ì¡°íšŒ.csv\"\n","\n","# í´ë” ë‚´ íŒŒì¼ ëª©ë¡ì„ ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬\n","def get_sorted_transaction_files():\n","    files = [f for f in os.listdir(transactions_folder) if f.endswith(\".tsv\")]\n","\n","    # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: blockchair_bitcoin_transactions_20250216.tsv â†’ 20250216)\n","    def extract_date(filename):\n","        match = re.search(r\"(\\d{8})\", filename)\n","        return int(match.group(1)) if match else 0\n","\n","    # ë‚ ì§œìˆœ ì •ë ¬\n","    sorted_files = sorted(files, key=extract_date)\n","\n","    return [os.path.join(transactions_folder, f) for f in sorted_files]\n","\n","# 10BTC ì´ìƒì¸ íŠ¸ëœì­ì…˜ í•„í„°ë§\n","def filter_large_transactions(file_path):\n","    df = pd.read_csv(file_path, sep='\\t')  # TSV íŒŒì¼ ì½ê¸°\n","    df_filtered = df[df['input_total'] >= 1_000_000_000]  # 10 BTC ì´ìƒ\n","    return df_filtered['hash'].tolist()\n","\n","# íŠ¸ëœì­ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (API ìš”ì²­)\n","def fetch_transaction(txid):\n","    url = f\"https://blockchain.info/rawtx/{txid}\"\n","    try:\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()\n","        return response.json()\n","    except requests.RequestException:\n","        print(f\"{get_current_timestamp()} âŒ íŠ¸ëœì­ì…˜ {txid} ì²˜ë¦¬ ì‹¤íŒ¨ (API ìš”ì²­ ì˜¤ë¥˜)\")\n","        return None\n","\n","# ë³‘ë ¬ë¡œ íŠ¸ëœì­ì…˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸° ë°©ì‹)\n","def get_transactions_data(txids):\n","    transactions = {}\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n","        futures = {executor.submit(fetch_transaction, txid): txid for txid in txids}\n","\n","        for future in concurrent.futures.as_completed(futures):\n","            txid = futures[future]\n","            try:\n","                data = future.result()\n","                if data:\n","                    transactions[txid] = data\n","            except Exception as e:\n","                print(f\"{get_current_timestamp()} âŒ íŠ¸ëœì­ì…˜ {txid} ì²˜ë¦¬ ì‹¤íŒ¨ (ì˜ˆì™¸ ë°œìƒ: {e})\")\n","\n","    return transactions\n","\n","# ë³‘ë ¬ë¡œ ì£¼ì†Œ ë§¤ì¹­ ë° ì…ì¶œê¸ˆ íŒë³„ (ë°°ì¹˜ ì²˜ë¦¬)\n","def process_address_batch(addresses_set, tx_data_batch):\n","    results = []\n","    for tx in tx_data_batch:\n","        matched_type = None\n","        inputs = {inp['prev_out']['addr'] for inp in tx.get('inputs', []) if 'prev_out' in inp}\n","        outputs = {out['addr'] for out in tx.get('out', []) if 'addr' in out}\n","\n","        if any(address in outputs for address in addresses_set):\n","            matched_type = \"deposit\"\n","        elif any(address in inputs for address in addresses_set):\n","            matched_type = \"withdrawal\"\n","\n","        results.append(matched_type)\n","    return results\n","\n","# ë³‘ë ¬ë¡œ íŠ¸ëœì­ì…˜ ë°°ì¹˜ ì²˜ë¦¬\n","def process_transaction_batches(addresses_set, tx_data, batch_size=500):\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n","        futures = []\n","        for i in range(0, len(tx_data), batch_size):\n","            tx_batch = tx_data[i:i + batch_size]\n","            futures.append(executor.submit(process_address_batch, addresses_set, tx_batch))\n","\n","        all_results = []\n","        for future in concurrent.futures.as_completed(futures):\n","            result = future.result()\n","            all_results.extend(result)\n","\n","    return all_results\n","\n","# í˜„ì¬ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n","def get_current_timestamp():\n","    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","# ë©”ì¸ ì‹¤í–‰ ë¡œì§\n","def main():\n","    print(f\"{get_current_timestamp()} ğŸ”„ íŠ¸ëœì­ì…˜ íŒŒì¼ ì •ë ¬ ì¤‘...\")\n","    transaction_files = get_sorted_transaction_files()\n","\n","    for file in transaction_files:\n","        print(f\"{get_current_timestamp()} ğŸ“‚ ì²˜ë¦¬ ì¤‘: {file}\")\n","\n","        txids = filter_large_transactions(file)\n","        print(f\"{get_current_timestamp()} ğŸš€ API ë³‘ë ¬ ìš”ì²­ ì‹¤í–‰ ì¤‘...\")\n","        transactions_data = get_transactions_data(txids)\n","\n","        print(f\"{get_current_timestamp()} ğŸ“‚ ìµœê·¼ 1ì¼ íŠ¸ëœì­ì…˜ CSV íŒŒì¼ ë¡œë“œ ì¤‘...\")\n","        df_recent = pd.read_csv(recent_blocks_file)\n","\n","        addresses_set = set(df_recent['address'])  # ì§‘í•©ìœ¼ë¡œ ë³€í™˜\n","        print(f\"{get_current_timestamp()} ğŸ“œ ì´ {len(addresses_set)}ê°œì˜ ì§€ê°‘ ì£¼ì†Œ ë¡œë“œ ì™„ë£Œ.\")\n","\n","        print(f\"{get_current_timestamp()} ğŸ” ì£¼ì†Œ ë§¤ì¹­ ë° ì…ì¶œê¸ˆ íŒë³„ ì¤‘...\")\n","\n","        all_results = process_transaction_batches(addresses_set, list(transactions_data.values()), batch_size=500)\n","\n","        for i, result in enumerate(all_results):\n","            if result:\n","                df_recent.at[i, 'transaction_type'] = result\n","\n","            if (i+1) % 1000 == 0:\n","                print(f\"{get_current_timestamp()} {i+1}/{len(df_recent)} ì£¼ì†Œ ë§¤ì¹­ ì²˜ë¦¬ ì¤‘...\")\n","\n","        print(f\"{get_current_timestamp()} ğŸ’¾ íŒŒì¼ ì—…ë°ì´íŠ¸ ì¤‘...\")\n","        df_recent.to_csv(recent_blocks_file, index=False)\n","        print(f\"{get_current_timestamp()} âœ… ì²˜ë¦¬ ì™„ë£Œ! ê¸°ì¡´ íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë¨: {recent_blocks_file}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"j-MnWfciqEbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import asyncio\n","import aiohttp\n","import csv\n","import json\n","import requests\n","import os\n","import pandas as pd\n","from pathlib import Path\n","from datetime import datetime\n","\n","# Bitcoin Core RPC ì„¤ì •\n","RPC_URL = \"http://localhost:8332\"\n","HEADERS = {\"content-type\": \"application/json\"}\n","COOKIE_PATH = Path(\"C:/Users/fhqhz/AppData/Roaming/Bitcoin/.cookie\")\n","CSV_FILE = Path(\"C:/python/(4-0) ìµœê·¼ ë¸”ë¡ ì¡°íšŒ.csv\")\n","BLOCKCHAIN_INFO_API = \"https://blockchain.info/rawtx/{}\"\n","\n","def log_time(message):\n","    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    print(f\"[{now}] {message}\")\n","\n","def get_rpc_auth():\n","    with open(COOKIE_PATH, \"r\") as f:\n","        return tuple(f.read().strip().split(\":\", 1))\n","\n","def get_latest_blocks(count=30):\n","    log_time(f\"ìµœê·¼ {count}ê°œ ë¸”ë¡ ì¡°íšŒ ì‹œì‘\")\n","    auth = get_rpc_auth()\n","\n","    payload = json.dumps({\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"getbestblockhash\", \"params\": []})\n","    response = requests.post(RPC_URL, headers=HEADERS, auth=auth, data=payload)\n","\n","    if response.status_code != 200:\n","        log_time(f\"getbestblockhash í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}\")\n","        return []\n","\n","    latest_block_hash = response.json().get(\"result\")\n","    if latest_block_hash is None:\n","        log_time(\"getbestblockhash ê²°ê³¼ê°€ Noneì„\")\n","        return []\n","\n","    blocks = []\n","    for _ in range(count):\n","        payload = json.dumps({\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"getblock\", \"params\": [latest_block_hash, 2]})\n","        response = requests.post(RPC_URL, headers=HEADERS, auth=auth, data=payload)\n","\n","        if response.status_code != 200:\n","            log_time(f\"getblock í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}\")\n","            break\n","\n","        block_data = response.json().get(\"result\")\n","        if block_data is None:\n","            log_time(\"getblock ì‘ë‹µì—ì„œ resultê°€ Noneì„\")\n","            break\n","\n","        blocks.append(block_data)\n","        latest_block_hash = block_data.get(\"previousblockhash\")\n","        if not latest_block_hash:\n","            break\n","\n","    log_time(f\"ìµœê·¼ {len(blocks)}ê°œ ë¸”ë¡ ì¡°íšŒ ì™„ë£Œ\")\n","    return blocks[::-1]\n","\n","def filter_large_transactions(transactions):\n","    log_time(\"10BTC ì´ìƒ ê±°ë˜ í•„í„°ë§ ì‹œì‘\")\n","    result = [tx[\"txid\"] for tx in transactions if sum(out[\"value\"] for out in tx.get(\"vout\", [])) >= 10]\n","    log_time(f\"10BTC ì´ìƒ ê±°ë˜ {len(result)}ê±´ í•„í„°ë§ ì™„ë£Œ\")\n","    return result\n","\n","async def fetch_transaction_data(session, txid):\n","    try:\n","        async with session.get(BLOCKCHAIN_INFO_API.format(txid)) as response:\n","            if response.status != 200:\n","                log_time(f\"íŠ¸ëœì­ì…˜ {txid} ì¡°íšŒ ì‹¤íŒ¨: {response.status}\")\n","                return None\n","            return await response.json()\n","    except Exception as e:\n","        log_time(f\"íŠ¸ëœì­ì…˜ {txid} ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n","        return None\n","\n","async def get_transaction_addresses(txids):\n","    log_time(\"íŠ¸ëœì­ì…˜ ì£¼ì†Œ ì¡°íšŒ ì‹œì‘\")\n","    addresses = {}\n","    async with aiohttp.ClientSession() as session:\n","        tasks = [fetch_transaction_data(session, txid) for txid in txids]\n","        results = await asyncio.gather(*tasks)\n","        for tx_data in results:\n","            if not tx_data:\n","                continue  # ì¡°íšŒ ì‹¤íŒ¨ ì‹œ íŒ¨ìŠ¤\n","            if \"inputs\" in tx_data:\n","                for inp in tx_data[\"inputs\"]:\n","                    if \"prev_out\" in inp and \"addr\" in inp[\"prev_out\"]:\n","                        addresses[inp[\"prev_out\"][\"addr\"]] = \"withdrawal\"\n","            if \"out\" in tx_data:\n","                for out in tx_data[\"out\"]:\n","                    if \"addr\" in out:\n","                        addresses[out[\"addr\"]] = \"deposit\"\n","    log_time(\"íŠ¸ëœì­ì…˜ ì£¼ì†Œ ì¡°íšŒ ì™„ë£Œ\")\n","    return addresses\n","\n","def update_csv(address_data):\n","    log_time(\"CSV íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹œì‘\")\n","    rows = []\n","    with open(CSV_FILE, \"r\", encoding=\"utf-8\") as f:\n","        reader = csv.DictReader(f)\n","        for row in reader:\n","            if row[\"address\"] in address_data:\n","                row[\"transaction_type\"] = address_data[row[\"address\"]]\n","            rows.append(row)\n","    with open(CSV_FILE, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","        writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n","        writer.writeheader()\n","        writer.writerows(rows)\n","    log_time(\"CSV íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n","\n","async def main():\n","    blocks = get_latest_blocks(30)\n","    all_txids = []\n","    for block in blocks:\n","        all_txids.extend(filter_large_transactions(block.get(\"tx\", [])))\n","\n","    address_data = await get_transaction_addresses(all_txids)\n","    update_csv(address_data)\n","    log_time(\"ì‘ì—… ì™„ë£Œ\")\n","\n","if __name__ == \"__main__\":\n","    asyncio.run(main())\n"],"metadata":{"id":"HwSJ3wonqEYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import requests\n","import concurrent.futures\n","from datetime import datetime\n","\n","# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n","transactions_file = \"C:/python/python(1-0)TSV/blockchair_bitcoin_transactions_20250309.tsv\"\n","recent_blocks_file = \"C:/python/(4-0) ìµœê·¼ ë¸”ë¡ ì¡°íšŒ.csv\"\n","\n","# 10BTC ì´ìƒì¸ íŠ¸ëœì­ì…˜ í•„í„°ë§\n","def filter_large_transactions():\n","    df = pd.read_csv(transactions_file, sep='\\t')  # TSV íŒŒì¼ ì½ê¸°\n","    df_filtered = df[df['input_total'] >= 1_000_000_000]  # 10 BTC ì´ìƒ\n","    return df_filtered['hash'].tolist()\n","\n","# íŠ¸ëœì­ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (API ìš”ì²­)\n","def fetch_transaction(txid):\n","    url = f\"https://blockchain.info/rawtx/{txid}\"\n","    try:\n","        response = requests.get(url, timeout=10)\n","        response.raise_for_status()\n","        return response.json()\n","    except requests.RequestException:\n","        print(f\"{get_current_timestamp()} âŒ íŠ¸ëœì­ì…˜ {txid} ì²˜ë¦¬ ì‹¤íŒ¨ (API ìš”ì²­ ì˜¤ë¥˜)\")\n","        return None\n","\n","# ë³‘ë ¬ë¡œ íŠ¸ëœì­ì…˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸° ë°©ì‹)\n","def get_transactions_data(txids):\n","    transactions = {}\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n","        futures = {executor.submit(fetch_transaction, txid): txid for txid in txids}\n","\n","        for future in concurrent.futures.as_completed(futures):\n","            txid = futures[future]\n","            try:\n","                data = future.result()\n","                if data:\n","                    transactions[txid] = data\n","            except Exception as e:\n","                print(f\"{get_current_timestamp()} âŒ íŠ¸ëœì­ì…˜ {txid} ì²˜ë¦¬ ì‹¤íŒ¨ (ì˜ˆì™¸ ë°œìƒ: {e})\")\n","\n","    return transactions\n","\n","# ì£¼ì†Œ ë§¤ì¹­ ë° ì…ì¶œê¸ˆ íŒë³„\n","def determine_transaction_type(address, tx_data):\n","    for tx in sorted(tx_data, key=lambda x: x['time'], reverse=True):  # ìµœì‹  íŠ¸ëœì­ì…˜ ìš°ì„ \n","        inputs = {inp['prev_out']['addr'] for inp in tx.get('inputs', []) if 'prev_out' in inp}\n","        outputs = {out['addr'] for out in tx.get('out', []) if 'addr' in out}\n","\n","        if address in outputs:\n","            return \"deposit\"\n","        elif address in inputs:\n","            return \"withdrawal\"\n","    return None  # ê²°ê³¼ ì—†ìŒ\n","\n","# ë³‘ë ¬ë¡œ ì£¼ì†Œ ë§¤ì¹­ ë° ì…ì¶œê¸ˆ íŒë³„ (ë°°ì¹˜ ì²˜ë¦¬)\n","def process_address_batch(addresses_set, tx_data_batch):\n","    results = []\n","    for tx in tx_data_batch:\n","        matched_type = None\n","        # íŠ¸ëœì­ì…˜ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì§‘í•©ìœ¼ë¡œ ë³€í™˜\n","        inputs = {inp['prev_out']['addr'] for inp in tx.get('inputs', []) if 'prev_out' in inp}\n","        outputs = {out['addr'] for out in tx.get('out', []) if 'addr' in out}\n","\n","        # ì£¼ì†Œê°€ ì¶œë ¥ì— ìˆìœ¼ë©´ deposit, ì…ë ¥ì— ìˆìœ¼ë©´ withdrawal\n","        if any(address in outputs for address in addresses_set):\n","            matched_type = \"deposit\"\n","        elif any(address in inputs for address in addresses_set):\n","            matched_type = \"withdrawal\"\n","\n","        results.append(matched_type)\n","    return results\n","\n","# ë³‘ë ¬ë¡œ íŠ¸ëœì­ì…˜ ë°°ì¹˜ ì²˜ë¦¬\n","def process_transaction_batches(addresses_set, tx_data, batch_size=500):\n","    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n","        # íŠ¸ëœì­ì…˜ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬\n","        futures = []\n","        for i in range(0, len(tx_data), batch_size):\n","            tx_batch = tx_data[i:i + batch_size]\n","            futures.append(executor.submit(process_address_batch, addresses_set, tx_batch))\n","\n","        # ê²°ê³¼ë¥¼ ì²˜ë¦¬\n","        all_results = []\n","        for future in concurrent.futures.as_completed(futures):\n","            result = future.result()\n","            all_results.extend(result)\n","\n","    return all_results\n","\n","# í˜„ì¬ ì‹œê°„ ê°€ì ¸ì˜¤ê¸°\n","def get_current_timestamp():\n","    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","\n","# ë©”ì¸ ì‹¤í–‰ ë¡œì§\n","def main():\n","    print(f\"{get_current_timestamp()} ğŸ”„ 10BTC ì´ìƒ íŠ¸ëœì­ì…˜ í•„í„°ë§ ì¤‘...\")\n","    txids = filter_large_transactions()\n","\n","    print(f\"{get_current_timestamp()} ğŸš€ API ë³‘ë ¬ ìš”ì²­ ì‹¤í–‰ ì¤‘...\")\n","    transactions_data = get_transactions_data(txids)\n","\n","    print(f\"{get_current_timestamp()} ğŸ“‚ ìµœê·¼ 1ì¼ íŠ¸ëœì­ì…˜ CSV íŒŒì¼ ë¡œë“œ ì¤‘...\")\n","    df_recent = pd.read_csv(recent_blocks_file)\n","\n","    addresses_set = set(df_recent['address'])  # ì§‘í•©ìœ¼ë¡œ ë³€í™˜\n","    print(f\"{get_current_timestamp()} ğŸ“œ ì´ {len(addresses_set)}ê°œì˜ ì§€ê°‘ ì£¼ì†Œ ë¡œë“œ ì™„ë£Œ.\")\n","\n","    print(f\"{get_current_timestamp()} ğŸ” ì£¼ì†Œ ë§¤ì¹­ ë° ì…ì¶œê¸ˆ íŒë³„ ì¤‘...\")\n","\n","    # íŠ¸ëœì­ì…˜ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬\n","    all_results = process_transaction_batches(addresses_set, list(transactions_data.values()), batch_size=500)\n","\n","    # ê²°ê³¼ ë°˜ì˜\n","    for i, result in enumerate(all_results):\n","        if result:\n","            df_recent.at[i, 'transaction_type'] = result\n","\n","        if (i+1) % 1000 == 0:  # 1000ê°œë§ˆë‹¤ ì¶œë ¥\n","            print(f\"{get_current_timestamp()} {i+1}/{len(df_recent)} ì£¼ì†Œ ë§¤ì¹­ ì²˜ë¦¬ ì¤‘...\")\n","\n","    print(f\"{get_current_timestamp()} ğŸ’¾ íŒŒì¼ ì—…ë°ì´íŠ¸ ì¤‘...\")\n","    df_recent.to_csv(recent_blocks_file, index=False)\n","    print(f\"{get_current_timestamp()} âœ… ì²˜ë¦¬ ì™„ë£Œ! ê¸°ì¡´ íŒŒì¼ì´ ì—…ë°ì´íŠ¸ë¨: \", recent_blocks_file)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"id":"B3KJGnmTqEV-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import asyncio\n","import aiohttp\n","import csv\n","import json\n","import requests\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","from pathlib import Path\n","from datetime import datetime\n","\n","# Bitcoin Core RPC ì„¤ì •\n","RPC_URL = \"http://localhost:8332\"\n","HEADERS = {\"content-type\": \"application/json\"}\n","COOKIE_PATH = Path(\"C:/Users/fhqhz/AppData/Roaming/Bitcoin/.cookie\")\n","CSV_FILE = Path(\"C:/python/(4-0) ìµœê·¼ ë¸”ë¡ ì¡°íšŒ.csv\")\n","VISUALIZATION_FOLDER = Path(\"C:/python/(4-1) ì‹œê°í™”/\")\n","VISUALIZATION_FILE = VISUALIZATION_FOLDER / \"ì „ì²´_ì‹œê°í™”.png\"\n","BLOCKCHAIN_INFO_API = \"https://blockchain.info/rawtx/{}\"\n","\n","# í´ë” ìë™ ìƒì„±\n","VISUALIZATION_FOLDER.mkdir(parents=True, exist_ok=True)\n","\n","def log_time(message):\n","    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","    print(f\"[{now}] {message}\")\n","\n","def get_rpc_auth():\n","    with open(COOKIE_PATH, \"r\") as f:\n","        return tuple(f.read().strip().split(\":\", 1))\n","\n","def get_latest_block():\n","    log_time(\"ìµœì‹  ë¸”ë¡ í•´ì‹œ ì¡°íšŒ ì‹œì‘\")\n","    auth = get_rpc_auth()\n","    payload = json.dumps({\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"getbestblockhash\", \"params\": []})\n","    response = requests.post(RPC_URL, headers=HEADERS, auth=auth, data=payload)\n","    log_time(\"ìµœì‹  ë¸”ë¡ í•´ì‹œ ì¡°íšŒ ì™„ë£Œ\")\n","    return response.json().get(\"result\")\n","\n","def get_block_transactions(block_hash):\n","    log_time(\"ë¸”ë¡ íŠ¸ëœì­ì…˜ ì¡°íšŒ ì‹œì‘\")\n","    auth = get_rpc_auth()\n","    payload = json.dumps({\"jsonrpc\": \"1.0\", \"id\": \"curltest\", \"method\": \"getblock\", \"params\": [block_hash, 2]})\n","    response = requests.post(RPC_URL, headers=HEADERS, auth=auth, data=payload)\n","    log_time(\"ë¸”ë¡ íŠ¸ëœì­ì…˜ ì¡°íšŒ ì™„ë£Œ\")\n","    return response.json().get(\"result\", {}).get(\"tx\", [])\n","\n","def filter_large_transactions(transactions):\n","    log_time(\"10BTC ì´ìƒ ê±°ë˜ í•„í„°ë§ ì‹œì‘\")\n","    result = [tx[\"txid\"] for tx in transactions if sum(out[\"value\"] for out in tx.get(\"vout\", [])) >= 10]\n","    log_time(f\"10BTC ì´ìƒ ê±°ë˜ {len(result)}ê±´ í•„í„°ë§ ì™„ë£Œ\")\n","    return result\n","\n","async def fetch_transaction_data(session, txid):\n","    url = BLOCKCHAIN_INFO_API.format(txid)\n","    try:\n","        async with session.get(url) as response:\n","            if response.status != 200:\n","                log_time(f\"API ì˜¤ë¥˜ {response.status}: {txid}\")\n","                return None  # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜\n","\n","            if \"application/json\" not in response.content_type:\n","                log_time(f\"ì˜ëª»ëœ ì‘ë‹µ íƒ€ì…: {response.content_type} - {txid}\")\n","                return None  # JSONì´ ì•„ë‹Œ ì‘ë‹µ ë¬´ì‹œ\n","\n","            return await response.json()\n","    except Exception as e:\n","        log_time(f\"ì˜ˆì™¸ ë°œìƒ: {e} - {txid}\")\n","        return None  # ì˜¤ë¥˜ ë°œìƒí•˜ë©´ None ë°˜í™˜\n","\n","\n","async def get_transaction_addresses(txids):\n","    log_time(\"íŠ¸ëœì­ì…˜ ì£¼ì†Œ ì¡°íšŒ ì‹œì‘\")\n","    addresses = {}\n","    async with aiohttp.ClientSession() as session:\n","        tasks = [fetch_transaction_data(session, txid) for txid in txids]\n","        results = await asyncio.gather(*tasks)\n","        for tx_data in results:\n","            if tx_data is None:\n","                continue  # None ê°’ì€ ê±´ë„ˆëœ€\n","            # ì…ë ¥ ì£¼ì†Œ í™•ì¸\n","            if \"inputs\" in tx_data:\n","                for inp in tx_data[\"inputs\"]:\n","                    if \"prev_out\" in inp and \"addr\" in inp[\"prev_out\"]:\n","                        addresses[inp[\"prev_out\"][\"addr\"]] = \"withdrawal\"\n","            # ì¶œë ¥ ì£¼ì†Œ í™•ì¸\n","            if \"out\" in tx_data:\n","                for out in tx_data[\"out\"]:\n","                    if \"addr\" in out:\n","                        addresses[out[\"addr\"]] = \"deposit\"\n","    log_time(\"íŠ¸ëœì­ì…˜ ì£¼ì†Œ ì¡°íšŒ ì™„ë£Œ\")\n","    return addresses\n","\n","def update_csv(address_data):\n","    log_time(\"CSV íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹œì‘\")\n","\n","    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ\n","    rows = []\n","    changes = {}  # ë³€ê²½ ì‚¬í•­ ì €ì¥\n","\n","    with open(CSV_FILE, \"r\", encoding=\"utf-8\") as f:\n","        reader = csv.DictReader(f)\n","        for row in reader:\n","            old_type = row[\"transaction_type\"]\n","            new_type = address_data.get(row[\"address\"], old_type)  # ì—†ìœ¼ë©´ ê¸°ì¡´ ê°’ ìœ ì§€\n","\n","            if old_type != new_type:\n","                key = f\"{row['prediction']}_{row['trader_type']}_{row['whale_type']}\"\n","\n","                # deposit ì¦ê°€ â +1, withdrawal ì¦ê°€ â -1\n","                if new_type == \"deposit\":\n","                    changes[key] = changes.get(key, 0) + 1\n","                elif new_type == \"withdrawal\":\n","                    changes[key] = changes.get(key, 0) - 1\n","\n","            row[\"transaction_type\"] = new_type\n","            rows.append(row)\n","\n","    # CSV íŒŒì¼ ì—…ë°ì´íŠ¸\n","    with open(CSV_FILE, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","        writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n","        writer.writeheader()\n","        writer.writerows(rows)\n","\n","    log_time(\"CSV íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ\")\n","\n","\n","\n","    # ë³€ê²½ ì‚¬í•­ ì¶œë ¥\n","    if changes:\n","        for key, change in changes.items():\n","            change_sign = \"+\" if change > 0 else \"\"\n","            print(f\"{key} : {change_sign}{change}\")\n","    else:\n","        print(\"ğŸ”¹ ë³€ê²½ëœ ë°ì´í„° ì—†ìŒ\")\n","\n","\n","\n","def visualize_data():\n","    log_time(\"ë°ì´í„° ì‹œê°í™” ì‹œì‘\")\n","    df = pd.read_csv(CSV_FILE, encoding=\"utf-8-sig\")\n","    trader_type_order = ['scalper', 'short swing', 'long swing', 'long term']\n","    whale_type_order = ['small', 'medium', 'large']\n","    df['trader_type'] = pd.Categorical(df['trader_type'], categories=trader_type_order, ordered=True)\n","    df['whale_type'] = pd.Categorical(df['whale_type'], categories=whale_type_order, ordered=True)\n","    df['prediction'] = df['prediction'].apply(lambda x: 'LONG' if x == 'LONG' else 'SHORT')\n","    df.sort_values(by=['prediction', 'whale_type', 'trader_type'], ascending=[False, True, True], inplace=True)\n","    combinations = df[['prediction', 'trader_type', 'whale_type']].drop_duplicates()\n","    num_plots = len(combinations)\n","    cols = 4\n","    rows = (num_plots + cols - 1) // cols\n","    fig, axes = plt.subplots(rows, cols, figsize=(10, 1 * rows))\n","    axes = axes.flatten()\n","    font_path = \"C:/Windows/Fonts/malgun.ttf\"\n","    prop = fm.FontProperties(fname=font_path)\n","    plt.rcParams['font.family'] = prop.get_name()\n","    plt.rcParams['axes.unicode_minus'] = False\n","    for idx, (_, row) in enumerate(combinations.iterrows()):\n","        if idx >= len(axes):\n","            break\n","        prediction, trader_type, whale_type = row\n","        filtered_data = df[(df['prediction'] == prediction) & (df['trader_type'] == trader_type) & (df['whale_type'] == whale_type)]\n","\n","        # None ê°’ ì œì™¸ (ì—¬ê¸° ì¶”ê°€)\n","        filtered_data = filtered_data[filtered_data['transaction_type'] != \"Non\"]\n","\n","        if filtered_data.empty:\n","            continue\n","        transaction_counts = filtered_data['transaction_type'].value_counts()\n","\n","        ax = axes[idx]\n","        transaction_counts = filtered_data['transaction_type'].value_counts()\n","        color_map = {'deposit': 'green', 'withdrawal': 'red'}\n","        colors = [color_map.get(tx_type, 'gray') for tx_type in transaction_counts.index]\n","\n","        transaction_counts.plot(kind='barh', color=colors, ax=ax)\n","\n","\n","        ax.set_title(f\"{prediction}_{trader_type}_{whale_type}\", fontsize=10)\n","        ax.set_xlabel('')\n","        ax.set_ylabel('')\n","\n","\n","    for idx in range(len(combinations), len(axes)):\n","        fig.delaxes(axes[idx])\n","    plt.tight_layout()\n","    plt.savefig(VISUALIZATION_FILE, dpi=300)\n","    plt.close()\n","    log_time(\"ë°ì´í„° ì‹œê°í™” ì™„ë£Œ\")\n","\n","\n","\n","async def main():\n","    while True:\n","        block_hash = get_latest_block()\n","        transactions = get_block_transactions(block_hash)\n","        large_txids = filter_large_transactions(transactions)\n","        address_data = await get_transaction_addresses(large_txids)\n","        update_csv(address_data)\n","        visualize_data()\n","        log_time(\"ëŒ€ê¸° ì¤‘...\")\n","        await asyncio.sleep(480)\n","\n","if __name__ == \"__main__\":\n","    asyncio.run(main())"],"metadata":{"id":"ekzs_EpVqETd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W5CYFae1qEQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vVWNj3u5qEON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jZQB8FmCqEKQ"},"execution_count":null,"outputs":[]}]}